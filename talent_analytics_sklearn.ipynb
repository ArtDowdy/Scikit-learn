{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpacwsPc1UcoXsQWBvHiSO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtDowdy/Scikit-learn/blob/main/talent_analytics_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d20UUfIk-w9D"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Talent Analytics (scikit-learn, Colab-ready)\n",
        "# - Synthetic ATS/HRIS data generator\n",
        "# - Candidate Success Classification (Calibrated, fairness audit)\n",
        "# - Time-to-Hire Regression\n",
        "# - Sourcing Channel ROI & Lift\n",
        "# - Funnel Anomaly Detection (Isolation Forest)\n",
        "# - Leakage-safe CV (GroupKFold by req_id)\n",
        "# - ColumnTransformer + Pipeline + RandomizedSearchCV\n",
        "# - Model export (joblib) + Model Card (JSON)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, math, random, string, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, IsolationForest\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, brier_score_loss, f1_score, accuracy_score,\n",
        "    mean_absolute_error, r2_score, confusion_matrix\n",
        ")\n",
        "from sklearn.inspection import permutation_importance\n",
        "import joblib\n",
        "np.random.seed(42); random.seed(42)\n",
        "\n",
        "print(\"Using:\", {\n",
        "    \"pandas\": pd.__version__,\n",
        "    \"numpy\": np.__version__,\n",
        "})\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Synthetic ATS/HRIS dataset\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class GenCfg:\n",
        "    n_reqs: int = 220           # open requisitions\n",
        "    n_cand: int = 12000         # applicants\n",
        "    roles: Tuple[str,...] = (\"ml_eng\",\"data_scientist\",\"analyst\",\"backend\",\"mobile\",\"designer\")\n",
        "    levels: Tuple[str,...] = (\"IC1\",\"IC2\",\"IC3\",\"IC4\")\n",
        "    sources: Tuple[str,...] = (\"referral\",\"linkedin\",\"career_site\",\"agency\",\"event\",\"internal\")\n",
        "    locations: Tuple[str,...] = (\"SF\",\"LA\",\"NY\",\"Remote\",\"Austin\",\"Seattle\")\n",
        "    genders: Tuple[str,...] = (\"F\",\"M\",\"NB\",\"Unknown\")\n",
        "    stages: Tuple[str,...] = (\"applied\",\"screen\",\"hm_interview\",\"loop\",\"offer\")\n",
        "\n",
        "CFG = GenCfg()\n",
        "\n",
        "def random_id(prefix, n=8):\n",
        "    return prefix + \"_\" + \"\".join(random.choices(string.ascii_lowercase+string.digits, k=n))\n",
        "\n",
        "def generate_data(cfg=CFG):\n",
        "    # requisitions with latent difficulty + urgency\n",
        "    reqs = []\n",
        "    for i in range(cfg.n_reqs):\n",
        "        role = random.choice(cfg.roles)\n",
        "        level = np.random.choice(cfg.levels, p=[0.25,0.35,0.25,0.15])\n",
        "        loc = np.random.choice(cfg.locations, p=[.23,.15,.22,.25,.08,.07])\n",
        "        diff = np.clip(np.random.beta(2,5), 0.05, 0.95)             # hiring difficulty (higher = harder)\n",
        "        urgency = np.random.uniform(0.2, 1.2)                        # >1 means rush\n",
        "        reqs.append({\"req_id\": random_id(\"REQ\"), \"role\": role, \"level\": level, \"loc\": loc,\n",
        "                     \"req_difficulty\": diff, \"urgency\": urgency})\n",
        "    reqs = pd.DataFrame(reqs)\n",
        "\n",
        "    # candidates\n",
        "    rows = []\n",
        "    for _ in range(cfg.n_cand):\n",
        "        r = reqs.sample(1).iloc[0]\n",
        "        source = np.random.choice(cfg.sources, p=[.22,.28,.25,.08,.07,.10])\n",
        "        gender = np.random.choice(cfg.genders, p=[.32,.50,.03,.15])\n",
        "        years_exp = np.clip(np.random.normal(5, 2.5), 0, 30)\n",
        "        top_school = np.random.binomial(1, 0.20)\n",
        "        skill_count = int(np.clip(np.random.normal(6, 2), 1, 20))\n",
        "        assessment = np.clip(np.random.normal(0.0, 1.0), -3, 3)      # standardized pre-hire test\n",
        "        recruiter_load = np.random.randint(8, 28)\n",
        "        comp_expect = np.random.normal(160, 30) * (1.15 if r[\"loc\"] in [\"SF\",\"NY\",\"LA\"] else 0.95)\n",
        "        # interview stage scores (missing if filtered earlier)\n",
        "        pass_screen = np.random.rand() < (0.55 + 0.1*top_school + 0.02*assessment - 0.25*r[\"req_difficulty\"])\n",
        "        hm_score = np.nan\n",
        "        loop_score = np.nan\n",
        "        if pass_screen:\n",
        "            hm_score = np.clip(np.random.normal(2.7 + 0.25*assessment + 0.04*skill_count, 0.8), 1, 5)\n",
        "            pass_hm = hm_score > (3.1 + 0.3*r[\"req_difficulty\"])\n",
        "            if pass_hm:\n",
        "                loop_score = np.clip(np.random.normal(3.0 + 0.3*assessment + 0.05*skill_count, 0.7), 1, 5)\n",
        "\n",
        "        # Offer probability depends on req difficulty + performance + urgency\n",
        "        offer_prob = 0.10 + 0.25*float(loop_score>3.2) + 0.12*float(hm_score and hm_score>3.2) \\\n",
        "                     + 0.05*top_school + 0.02*skill_count - 0.25*r[\"req_difficulty\"] + 0.06*(r[\"urgency\"]-0.8)\n",
        "        offer_prob += 0.05 if source==\"referral\" else 0\n",
        "        offer_prob = np.clip(offer_prob, 0.01, 0.95)\n",
        "        offer = np.random.rand() < offer_prob\n",
        "\n",
        "        # Accept offer probability & future success (retention+performance proxy)\n",
        "        accept_prob = 0.45 + 0.15*(offer) - 0.0015*max(comp_expect-170,0) + 0.05*(source==\"referral\")\n",
        "        accept = (np.random.rand() < np.clip(accept_prob,0.02,0.95))\n",
        "        # Success label (what we want to predict): depends on interview signals & req difficulty; slight group skew\n",
        "        success = (np.random.rand() <\n",
        "                   np.clip(0.35 + 0.18*float(loop_score>3.3) + 0.12*float(hm_score and hm_score>3.4)\n",
        "                           + 0.05*top_school + 0.01*skill_count - 0.15*r[\"req_difficulty\"], 0.02, 0.98))\n",
        "        # time-to-hire in days (if hired), else NaN; depends on urgency, recruiter_load, source\n",
        "        if accept:\n",
        "            tth = np.random.gamma(shape=3.0, scale=5.5) * (1.2 + 0.01*recruiter_load) * (1.1 if source==\"agency\" else 0.95)\n",
        "            tth *= (0.9 if r[\"urgency\"]>1.0 else 1.1)\n",
        "        else:\n",
        "            tth = np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"req_id\": r[\"req_id\"], \"role\": r[\"role\"], \"level\": r[\"level\"], \"loc\": r[\"loc\"],\n",
        "            \"source\": source, \"gender\": gender,\n",
        "            \"years_exp\": years_exp, \"top_school\": top_school, \"skill_count\": skill_count,\n",
        "            \"assessment_z\": assessment, \"recruiter_load\": recruiter_load,\n",
        "            \"comp_expect_k\": comp_expect, \"hm_score\": hm_score, \"loop_score\": loop_score,\n",
        "            \"offer\": int(offer), \"accept\": int(accept),\n",
        "            \"success_label\": int(success), \"time_to_hire_days\": tth,\n",
        "            \"req_difficulty\": r[\"req_difficulty\"], \"urgency\": r[\"urgency\"]\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    # introduce missingness/realism\n",
        "    for col in [\"hm_score\",\"loop_score\"]:\n",
        "        df[col] = df[col].astype(float)\n",
        "    return df, reqs\n",
        "\n",
        "df, reqs = generate_data()\n",
        "print(\"Data shape:\", df.shape)\n",
        "df.head(3)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2) Candidate Success Classifier (sklearn)\n",
        "# -----------------------------------------\n",
        "TARGET = \"success_label\"\n",
        "GROUP = \"req_id\"  # to avoid leakage between train/test splits\n",
        "CATS = [\"role\",\"level\",\"loc\",\"source\",\"gender\"]\n",
        "NUMS = [\"years_exp\",\"top_school\",\"skill_count\",\"assessment_z\",\"recruiter_load\",\n",
        "        \"comp_expect_k\",\"hm_score\",\"loop_score\",\"req_difficulty\",\"urgency\"]\n",
        "\n",
        "# Simple imputation (median for numeric, most_frequent for cats) via pipeline steps\n",
        "numeric_tf = Pipeline(steps=[\n",
        "    (\"impute\",  # use SimpleImputer without importing directly to keep deps minimal\n",
        "     type(\"MedianImputer\",(object,),{\"fit\":lambda self,X,y=None:self,\n",
        "                                     \"transform\":lambda self,X: pd.DataFrame(X).fillna(pd.DataFrame(X).median()).values})()),\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "categorical_tf = Pipeline(steps=[\n",
        "    (\"impute\",\n",
        "     type(\"ModeImputer\",(object,),{\"fit\":lambda self,X,y=None:self,\n",
        "                                   \"transform\":lambda self,X: pd.DataFrame(X).fillna(pd.DataFrame(X).mode().iloc[0]).values})()),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "pre = ColumnTransformer([\n",
        "    (\"num\", numeric_tf, NUMS),\n",
        "    (\"cat\", categorical_tf, CATS)\n",
        "])\n",
        "\n",
        "base_clf = LogisticRegression(max_iter=200, n_jobs=None)  # base model for calibrated wrapper\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "    (\"pre\", pre),\n",
        "    (\"clf\", CalibratedClassifierCV(\n",
        "        base_estimator=base_clf, method=\"sigmoid\", cv=3  # Platt scaling\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Hyperparameter search on a *different* classifier inside pipeline:\n",
        "# We'll swap to RandomForest inside CV using set_params trick.\n",
        "rf_search_space = {\n",
        "    \"clf__base_estimator\": [RandomForestClassifier()],\n",
        "    \"clf__base_estimator__n_estimators\": [200, 400],\n",
        "    \"clf__base_estimator__max_depth\": [None, 10, 20],\n",
        "    \"clf__base_estimator__min_samples_leaf\": [1, 2, 4],\n",
        "    \"clf__base_estimator__class_weight\": [\"balanced\", None],\n",
        "}\n",
        "\n",
        "# Leakage-safe splits: group by req_id\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_distributions=rf_search_space,\n",
        "    n_iter=8,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=gkf.split(df, df[TARGET], groups=df[GROUP]),\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "search.fit(df[CATS+NUMS], df[TARGET])\n",
        "clf = search.best_estimator_\n",
        "print(\"Best AUC (CV):\", search.best_score_)\n",
        "print(\"Best params:\", search.best_params_)\n",
        "\n",
        "# Evaluate on a fresh split by req_id (holdout)\n",
        "unique_reqs = df[GROUP].unique()\n",
        "np.random.shuffle(unique_reqs)\n",
        "split = int(0.8*len(unique_reqs))\n",
        "req_train, req_test = unique_reqs[:split], unique_reqs[split:]\n",
        "tr = df[df[GROUP].isin(req_train)]\n",
        "te = df[df[GROUP].isin(req_test)]\n",
        "\n",
        "clf.fit(tr[CATS+NUMS], tr[TARGET])\n",
        "probs = clf.predict_proba(te[CATS+NUMS])[:,1]\n",
        "preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "auc = roc_auc_score(te[TARGET], probs)\n",
        "ap = average_precision_score(te[TARGET], probs)\n",
        "brier = brier_score_loss(te[TARGET], probs)\n",
        "acc = accuracy_score(te[TARGET], preds)\n",
        "f1 = f1_score(te[TARGET], preds)\n",
        "print(f\"[Holdout] AUC={auc:.3f}  AP={ap:.3f}  Brier={brier:.3f}  Acc={acc:.3f}  F1={f1:.3f}\")\n",
        "\n",
        "# Feature importances via permutation (post-preprocessing)\n",
        "perm = permutation_importance(clf, te[CATS+NUMS], te[TARGET], n_repeats=5, random_state=42, n_jobs=-1)\n",
        "imp = pd.DataFrame({\"feature\": CATS+NUMS, \"importance\": perm.importances_mean}).sort_values(\"importance\", ascending=False)\n",
        "print(\"\\nTop features (permutation):\\n\", imp.head(10))\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3) Lightweight Fairness Audit (group metrics)\n",
        "# -----------------------------------------\n",
        "def fairness_report(y_true, y_prob, group, threshold=0.5):\n",
        "    dfm = pd.DataFrame({\"y\": y_true, \"p\": y_prob, \"g\": group})\n",
        "    out = []\n",
        "    for g in dfm[\"g\"].unique():\n",
        "        d = dfm[dfm[\"g\"]==g]\n",
        "        sel_rate = (d[\"p\"]>=threshold).mean()\n",
        "        tpr = ((d[\"p\"]>=threshold) & (d[\"y\"]==1)).sum() / max((d[\"y\"]==1).sum(), 1)\n",
        "        fpr = ((d[\"p\"]>=threshold) & (d[\"y\"]==0)).sum() / max((d[\"y\"]==0).sum(), 1)\n",
        "        out.append({\"group\": g, \"selection_rate\": sel_rate, \"TPR\": tpr, \"FPR\": fpr, \"count\": len(d)})\n",
        "    rep = pd.DataFrame(out)\n",
        "    # gaps vs. global\n",
        "    base = rep[\"selection_rate\"].mean()\n",
        "    rep[\"sel_rate_gap\"] = rep[\"selection_rate\"] - base\n",
        "    rep[\"tpr_gap_to_max\"] = rep[\"TPR\"] - rep[\"TPR\"].max()\n",
        "    rep[\"fpr_gap_to_min\"] = rep[\"FPR\"] - rep[\"FPR\"].min()\n",
        "    return rep.sort_values(\"group\")\n",
        "\n",
        "fair = fairness_report(te[TARGET].values, probs, te[\"gender\"].values, threshold=0.5)\n",
        "print(\"\\nFairness report by gender:\\n\", fair)\n",
        "\n",
        "# Optional: tune threshold to equalize TPR across groups (quick demo)\n",
        "best_tau, best_disp = 0.5, 1e9\n",
        "for tau in np.linspace(0.3, 0.7, 21):\n",
        "    fr = fairness_report(te[TARGET].values, probs, te[\"gender\"].values, threshold=tau)\n",
        "    disp = fr[\"TPR\"].max() - fr[\"TPR\"].min()\n",
        "    if disp < best_disp:\n",
        "        best_disp, best_tau = disp, tau\n",
        "print(f\"\\nSuggested threshold for smaller TPR disparity: tauâ‰ˆ{best_tau:.2f} (dispersion={best_disp:.3f})\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4) Time-to-Hire Regression (if hired)\n",
        "# -----------------------------------------\n",
        "hired = df[df[\"accept\"]==1].copy()\n",
        "REG_TARGET = \"time_to_hire_days\"\n",
        "reg_cats = [\"role\",\"level\",\"loc\",\"source\"]\n",
        "reg_nums = [\"years_exp\",\"skill_count\",\"assessment_z\",\"recruiter_load\",\"req_difficulty\",\"urgency\"]\n",
        "pre_reg = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), reg_nums),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), reg_cats)\n",
        "])\n",
        "reg_pipe = Pipeline([\n",
        "    (\"pre\", pre_reg),\n",
        "    (\"reg\", GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "reg_param = {\n",
        "    \"reg__n_estimators\": [200, 300],\n",
        "    \"reg__max_depth\": [2,3],\n",
        "    \"reg__learning_rate\": [0.05, 0.1],\n",
        "}\n",
        "reg_search = RandomizedSearchCV(reg_pipe, reg_param, n_iter=4, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1, random_state=42)\n",
        "reg_search.fit(hired[reg_cats+reg_nums], hired[REG_TARGET])\n",
        "reg = reg_search.best_estimator_\n",
        "pred_tth = reg.predict(hired[reg_cats+reg_nums])\n",
        "mae = mean_absolute_error(hired[REG_TARGET], pred_tth)\n",
        "r2 = r2_score(hired[REG_TARGET], pred_tth)\n",
        "print(f\"\\nTime-to-Hire  |  MAE={mae:.2f} days  R2={r2:.3f}  (n={len(hired)})\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 5) Sourcing Channel ROI / Lift (simple)\n",
        "# -----------------------------------------\n",
        "channel = te.groupby(\"source\").apply(lambda d: pd.Series({\n",
        "    \"n\": len(d),\n",
        "    \"offer_rate\": d[\"offer\"].mean(),\n",
        "    \"accept_rate\": d[\"accept\"].mean(),\n",
        "    \"success_rate\": d[TARGET].mean(),\n",
        "})).reset_index()\n",
        "channel[\"success_lift_vs_avg\"] = channel[\"success_rate\"] - te[TARGET].mean()\n",
        "channel = channel.sort_values(\"success_rate\", ascending=False)\n",
        "print(\"\\nChannel effectiveness (holdout):\\n\", channel)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 6) Funnel Anomaly Detection (per req)\n",
        "# -----------------------------------------\n",
        "# Build per-req funnel metrics, then detect anomalies in conversion patterns\n",
        "funnel = df.groupby(\"req_id\").agg(\n",
        "    applied=(\"req_id\",\"size\"),\n",
        "    screens=(\"hm_score\", lambda s: s.notna().sum()),\n",
        "    loops=(\"loop_score\", lambda s: s.notna().sum()),\n",
        "    offers=(\"offer\",\"sum\"),\n",
        "    accepts=(\"accept\",\"sum\"),\n",
        "    successes=(TARGET,\"sum\"),\n",
        "    recruiter_load=(\"recruiter_load\",\"mean\"),\n",
        "    diff=(\"req_difficulty\",\"mean\"),\n",
        "    urg=(\"urgency\",\"mean\")\n",
        ").reset_index()\n",
        "for a,b,name in [(\"screens\",\"applied\",\"screen_rate\"),\n",
        "                 (\"loops\",\"screens\",\"loop_rate\"),\n",
        "                 (\"offers\",\"loops\",\"offer_rate\"),\n",
        "                 (\"accepts\",\"offers\",\"accept_rate\"),\n",
        "                 (\"successes\",\"accepts\",\"success_rate\")]:\n",
        "    funnel[name] = np.where(funnel[b]>0, funnel[a]/funnel[b], 0.0)\n",
        "\n",
        "iso = IsolationForest(random_state=42, contamination=0.08)\n",
        "X_iso = funnel[[\"screen_rate\",\"loop_rate\",\"offer_rate\",\"accept_rate\",\"success_rate\",\"recruiter_load\",\"diff\",\"urg\"]]\n",
        "funnel[\"anomaly_score\"] = -iso.fit_score(X_iso)  # higher => more anomalous\n",
        "sus = funnel.sort_values(\"anomaly_score\", ascending=False).head(10)\n",
        "print(\"\\nPotential funnel anomalies (top 10 reqs):\\n\", sus[[\"req_id\",\"screen_rate\",\"loop_rate\",\"offer_rate\",\"accept_rate\",\"success_rate\",\"anomaly_score\"]])\n",
        "\n",
        "# -----------------------------------------\n",
        "# 7) Export artifacts + model card\n",
        "# -----------------------------------------\n",
        "os.makedirs(\"artifacts_talent\", exist_ok=True)\n",
        "clf_path = \"artifacts_talent/candidate_success_calibrated.joblib\"\n",
        "reg_path = \"artifacts_talent/time_to_hire_regressor.joblib\"\n",
        "joblib.dump(clf, clf_path); joblib.dump(reg, reg_path)\n",
        "\n",
        "card = {\n",
        "    \"task\": {\n",
        "        \"candidate_success_classification\": {\n",
        "            \"cv_best_auc\": float(search.best_score_),\n",
        "            \"holdout\": {\"auc\": float(auc), \"ap\": float(ap), \"brier\": float(brier), \"accuracy\": float(acc), \"f1\": float(f1)},\n",
        "            \"fairness\": fair.to_dict(orient=\"records\"),\n",
        "            \"suggested_threshold_for_tpr_balance\": float(best_tau),\n",
        "            \"top_features_permutation\": imp.head(10).to_dict(orient=\"records\"),\n",
        "            \"model_path\": clf_path,\n",
        "        },\n",
        "        \"time_to_hire_regression\": {\n",
        "            \"mae_days\": float(mae), \"r2\": float(r2),\n",
        "            \"model_path\": reg_path\n",
        "        },\n",
        "        \"sourcing_channel_effectiveness\": channel.to_dict(orient=\"records\"),\n",
        "        \"funnel_anomalies_top10\": sus[[\"req_id\",\"anomaly_score\"]].to_dict(orient=\"records\"),\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"rows\": int(len(df)),\n",
        "        \"requisitions\": int(len(reqs)),\n",
        "        \"note\": \"All data is synthetic; no personal/sensitive real-world data.\"\n",
        "    },\n",
        "    \"pipeline\": {\n",
        "        \"leakage_avoidance\": \"GroupKFold by req_id; independent holdout by req_id\",\n",
        "        \"preprocessing\": {\n",
        "            \"numeric\": [\"median impute\", \"StandardScaler\"],\n",
        "            \"categorical\": [\"mode impute\", \"OneHotEncoder(handle_unknown=ignore)\"]\n",
        "        },\n",
        "        \"calibration\": \"Platt scaling via CalibratedClassifierCV(cv=3)\",\n",
        "        \"hyperparam_search\": \"RandomizedSearchCV over RandomForest inside calibrated wrapper\"\n",
        "    }\n",
        "}\n",
        "with open(\"artifacts_talent/model_card.json\",\"w\") as f:\n",
        "    json.dump(card, f, indent=2)\n",
        "\n",
        "print(\"\\nSaved artifacts:\")\n",
        "print(\" -\", clf_path)\n",
        "print(\" -\", reg_path)\n",
        "print(\" - artifacts_talent/model_card.json\")\n",
        "\n",
        "# ----------------------\n"
      ]
    }
  ]
}